{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandonian/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rethinking the Image (Class)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am completely abandoning the idea of just thinking about tissue microarrays, spots etc...Rather, I want the image class to represent more of a multi-modal image fusion object. I imagine an image class that represents a set of spatial mappings from locations in a 2D grid (and hopefully 3D grid eventually) to any number of arbitrary features. The cannonical example would be an actual light miscroscopy image, where points in a 2D plane are mapped to light intensities in the visual spectrum. But, of course, there is no reason to stop there. My goal is for the image class to be able to not only accomodate, but \"fuse\" information obtained from as many different imaging modalities as possible (IHC, H&E, mass-spec, fluorescent etc.). I find the addition of (F)ISH images particularly compelling because, with the help of the Allen Brain Institue, we could (and hopefully will) develop a spatially-preserving map of gene expression levels for a large fraction of the mouse gennome. The advantage with this overal approach is that we would be able to develop a more detailed description of the \"state\" of a tissue, while making a point to take advantage of the strong suits of each imaging modality. Another advantage to this approach is that it would allow us to populate our tissue state descriptions quickly and directly from experiments. We are already seeing the beginnnings of this approach with a recent paper combining light microscopy and mass spec.[author]'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update (7/1/16):\n",
    "The overarching theme of the image class is 'fusion' at multiple scales: \n",
    "#### Fusion at the level of the data:\n",
    "<li>Combining images across imaging modalities</li>\n",
    "<li>Combining image data in 2D plane</li>\n",
    "<li>Combining image data in 3D space</li>\n",
    "\n",
    "#### Fusion of approaches:\n",
    "<li>Use the approach use by mass spec paper to combine imaging modalities</li>\n",
    "<li>Use Thunder for distributed data IO and processing</li>\n",
    "<li>Use Patch-based EM based fusion model to determine which patches of images are discriminative</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update (7/2/16):\n",
    "Upon closer inspection, it turns out that Thunder does exactly what I wanted to do, but for more traditional images, which is great news! I will clone their repository and add my multi-modal fusion idea to the existing code base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level implementation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation of tissue state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that a tissue's 'state' would be represented with a massive multidemensional array - a rectangular volume consisting of images/features layered back-to-back. The first 2 dimensions (height and width) would be used to preserve spatial information. The 3rd dimension consists of all the imaging modality channels. Adding additional features would just be a matter of appending an additional layer (2D-array) to this state 'volume'. As an example, one \"feature\" of a traditional image would be the intensity of red light at each location in space. Using this terminology, the images we are used to seeing are have green and blue features as well. In "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readout of tissue state:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional data visualization techniques could be used to visualize individual (or small subset <= 3) layers of a tissue state blob. Various spatial statistics could be used to summarize the features.channels that maintain a high degree of spatial specificity, while other techniques could be used to summarize layers that are adept at measuring other charactieristics (e.g. chemical specificity). For all layers, standard dimensionality reduction techniques could be used distill this state matrix down to its essential components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of tissue state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO: Use conolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long term goals/ideas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize measurement/images etc.\n",
    "For images modalities with\n",
    "standardize these images to a common coordinate frame:\n",
    "for \n",
    "then we can make measurements of a tissue's state at many time intervals. We essentially record the progression of some tissue's state (or more generally the state of anything that can be imaged/ measured with these modalities) over time. This makes would pave the way for studying disease progression at a huge scale at unprecedented levels of granularity. For example, we can use recurrent neural networks to characterize, classifiy and predict the progression of a disease. \n",
    "Once we have \n",
    "You can imagine that his approach can be extened to represent 3D volumes of tissue by stiching together images taken at various depths of a tissue block. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation behind this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am a firm believer that the relationship between man and machine will become increasingly symbiotic as we move forward, that medicine and biology will become increasingly quantitative and that fundamental discovers will increasingly depend on a machine's ability to leaern. I've heard over and over that we are entering an age of \"big data\". The most obvious consequence of this is that it's currently impossible for a single human being (or group) to take into consideration all the components. Many, if not all biological processes, are a complex orchestration perfromed by nature and governed by seemingly fundamental laws. There is a firm limit to the amount of information a human can concurrently reason about while the processing power of machines continues to grow. If nothing else, we will simply run out of meaningful labels for all the components of the system, (indexing components by number can be meaningful to computers, but not to humans unless they can remember the component to which the index points or have a lookup table that maps the index to more information- of course a computer does this, but much much faster). Subset of scientists must step away from studying the very specific to figure out how we can make computers handle the minutiae of scientific work.\n",
    "\n",
    " \n",
    "\n",
    "There are a few abilites that humans are much better at (or, are currently very difficult to implement in a computer): insight, intuition - which I think is really just more unconsious pattern recognition (and creativity) - this tendency to  produce oranized, clever and intelligent work seemingly spontaneuosly (i.e. not by exhaustively testing the entire set of possibilities, but by just exhibiting certain tendencies - I would argue that these are enitirely \"programmed\" by our environment and we already begin to see scientists begin to explain such phenomenom under on the basis of Long Term Potentiation). It's clear that \n",
    "\n",
    "\n",
    "To date, it appears that neural networks are our best approximation to building a machine that learns like a human/ best and most general approximate of how a human learns and recognizes patterns -\n",
    "stochasticiy of biochemical processes drives creativity and our ability to improve.even when humans try to produce the same set of behaviors, the product is slightly different each time - perhaps abberant synapse forms, and produces a slightly modified line of reasoning, leads to the development of a slightly novel idea and thus creativity is born. A more concrete example is skill learning. Skill learning is largely mediated by the basal ganglia, which is responsible for disinhibitir motor actions. Learning, say a golf swing, requires the individual to slightly modifiy his/her motor output until the desired swing is obtained. If the basal ganglia did not produce slight variations each time a swing was made, learning could not occur.\n",
    "This idea directly parallels a neural network, whereby during each iteration (or swing), the network tests to see how well it performed (evaluating a loss function) and make a small, slightly random adjustment (stochastic gradient descent) until the desired output is obtained. At a first glance, it appears that the most significant difference between a biological and artifical neural network is the complexity with which they are arranged. So it seems like future neural network practitioners will be tasked with finding clever and interesting ways to connect these networks, taking cues from biology as inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Clinial Applications:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immediate goals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Develop barebone skeleton for the image class plus utility, visualization, statistics and CNN classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<b>Proof of concept:</b>Let's see some code! A first pass at a  primitive implemention with just one layer of features is shown below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TODO:\n",
    "<li> Modify imageProvider to use image reader and writer to load images</li>\n",
    "<li> Modify Images to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"ImagePipeline.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
